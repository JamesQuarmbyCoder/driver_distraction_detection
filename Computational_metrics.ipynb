{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "import psutil\n",
    "# model = load_model(r\"C:/Users/james/Desktop/Diss_Code_for_AUC/CNN/Benchmark Models/auc_InceptionV3_Final.h5\")\n",
    "# model = load_model(r\"C:/Users/james/Desktop/Diss_Code_for_AUC/CNN/Benchmark Models/auc_ResNet50_Final.h5\")\n",
    "\n",
    "model = load_model(r\"C:/Users/james/Desktop/Diss_Code_for_AUC/CNN/CNN_Models/CNN_AUC_Final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20251 files belonging to 10 classes.\n",
      "Using 2025 files for validation.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (128, 128)\n",
    "val_split = 0.95\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_dir = r\"C:/Users/james/Desktop/Diss_Code_for_AUC/state-farm-distracted-driver-detection/imgs/train/\"\n",
    "SF_test_dataset = image_dataset_from_directory(train_dir,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE, \n",
    "                                             validation_split= 0.1, \n",
    "                                             subset= \"validation\", \n",
    "                                             seed = 1, \n",
    "                                             labels=\"inferred\")\n",
    "\n",
    "def normalize(image, label):\n",
    "    image = tf.cast(image/255.0, tf.float32)\n",
    "    return image, label\n",
    "SF_test_dataset = SF_test_dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect Computational Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 53ms/step\n",
      "2/2 [==============================] - 0s 54ms/step\n",
      "2/2 [==============================] - 0s 31ms/step\n",
      "2/2 [==============================] - 0s 32ms/step\n",
      "2/2 [==============================] - 0s 43ms/step\n",
      "2/2 [==============================] - 0s 28ms/step\n",
      "2/2 [==============================] - 0s 48ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 44ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 37ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 38ms/step\n",
      "2/2 [==============================] - 0s 40ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 39ms/step\n",
      "2/2 [==============================] - 0s 41ms/step\n",
      "2/2 [==============================] - 0s 36ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import psutil\n",
    "memory_usage_percent_list = []\n",
    "memory_usage_list = []\n",
    "cpu_usage_list = []\n",
    "latency_list = []\n",
    "for images, labels in SF_test_dataset:\n",
    "    start = time.time()\n",
    "    model.predict(images)\n",
    "    end = time.time()\n",
    "    latency_list.append(end - start)\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_usage_percent_list.append(memory_info.percent)\n",
    "    memory_usage_list.append((memory_info.used)/1024**2)\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    cpu_usage_list.append(cpu_percent)\n",
    "\n",
    "average_latency = np.mean(latency_list) #seconds\n",
    "\n",
    "average_latency_milliseconds = average_latency * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Latency for each Prediction is: 137.98 ms\n"
     ]
    }
   ],
   "source": [
    "OUT_Str = \"The Average Latency for each Prediction is: {:.2f} ms\".format(average_latency_milliseconds)\n",
    "print(OUT_Str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Throughput is: 7.25 predictions per second\n"
     ]
    }
   ],
   "source": [
    "count = len(latency_list)\n",
    "total_time = sum(latency_list)\n",
    "\n",
    "Throughput = count/total_time\n",
    "\n",
    "OUT_Str = \"The Throughput is: {:.2f} predictions per second\".format(Throughput)\n",
    "print(OUT_Str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average RAM Memory Usage is: 10811.35MB \n"
     ]
    }
   ],
   "source": [
    "Memory_usage_total = np.mean(memory_usage_list)\n",
    "OUT_Str = \"The Average RAM Memory Usage is: {:.2f}MB \".format(Memory_usage_total)\n",
    "print(OUT_Str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average % RAM Memory Usage is: 67.26% \n"
     ]
    }
   ],
   "source": [
    "Memory_usage = np.mean(memory_usage_percent_list)\n",
    "OUT_Str = \"The Average % RAM Memory Usage is: {:.2f}% \".format(Memory_usage)\n",
    "print(OUT_Str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average CPU Usage is: 9.83%\n"
     ]
    }
   ],
   "source": [
    "CPU_usage = np.mean(cpu_usage_list)\n",
    "OUT_Str = \"The Average CPU Usage is: {:.2f}%\".format(CPU_usage)\n",
    "print(OUT_Str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
